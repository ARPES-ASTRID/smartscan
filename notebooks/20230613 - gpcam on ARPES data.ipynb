{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import copy as cp\n",
    "from gpcam.gp_optimizer import fvGPOptimizer\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import default_rng\n",
    "import time\n",
    "from typing import *\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import dataloader as dl \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aqf_multid(x, gp):\n",
    "    a = 2.0  #####change here, 3.0 for 95 percent confidence interval\n",
    "    norm = 1.0\n",
    "    ret = None\n",
    "    for i in range(gp.input_dim-1):\n",
    "        x_pred=np.c_[x,np.zeros(x.shape[0])+i].reshape(-1,gp.input_dim)\n",
    "        cov = gp.posterior_covariance(x_pred)[\"v(x)\"]\n",
    "        if ret is None:\n",
    "            ret = cov\n",
    "        else:\n",
    "            ret += cov\n",
    "            \n",
    "    ret=a * np.sqrt(ret)\n",
    "\n",
    "    for i in range(gp.input_dim-1):\n",
    "        x_pred=np.c_[x,np.zeros(x.shape[0])+i].reshape(-1,gp.input_dim)\n",
    "        mean = gp.posterior_mean(x_pred)[\"f(x)\"]\n",
    "        ret += norm * mean\n",
    "\n",
    "    return ret\n",
    "\n",
    "def init_gp(\n",
    "        points,\n",
    "        values,\n",
    "        index_set_bounds,\n",
    "        hyperparameter_bounds,\n",
    "        hps_guess,\n",
    "        vp,\n",
    "        device:Literal['cpu','gpu']='cpu',\n",
    "    ) -> fvGPOptimizer:\n",
    "    gp = fvGPOptimizer(input_space_dimension=2, output_space_dimension=1, output_number=2, input_space_bounds=index_set_bounds, )\n",
    "    gp.tell(points,values,value_positions=vp)\n",
    "    gp.init_fvgp(hps_guess,compute_device=device)\n",
    "    gp.train_gp(hyperparameter_bounds,pop_size = 20,tolerance = 1e-6,max_iter = 2)\n",
    "    return gp\n",
    "\n",
    "def find_next(\n",
    "        points,\n",
    "        values,\n",
    "        gp,\n",
    "        hyperparameter_bounds,\n",
    "        maxnum:int=None,\n",
    "        dask_client=None,\n",
    "        method:Literal['global','local']='global',\n",
    "    ) -> Tuple[int,int]:\n",
    "    if maxnum is None:\n",
    "        gp.tell(points,values)\n",
    "    else:\n",
    "        gp.tell(points[-maxnum:],values[-maxnum:])\n",
    "    gp.train_gp(\n",
    "        hyperparameter_bounds=hyperparameter_bounds,\n",
    "        pop_size = 20,\n",
    "        method=method,\n",
    "        tolerance = 1e-6,\n",
    "        max_iter = 2,\n",
    "        dask_client = dask_client,\n",
    "        # device=device,\n",
    "    )\n",
    "        # gp.train_gp_async(hyperparameter_bounds, max_iter = 10000, dask_client = None)\n",
    "\n",
    "    new = gp.ask(\n",
    "        position = None, \n",
    "        n = 1, \n",
    "        acquisition_function = aqf_multid, \n",
    "        bounds = None,\n",
    "        method=method, \n",
    "        pop_size = 20, \n",
    "        max_iter = 20, \n",
    "        tol = 10e-6, \n",
    "        x0 = None, \n",
    "        dask_client = dask_client,\n",
    "        # device=device,\n",
    "    )\n",
    "    x,y = np.round(new['x'][0]).astype(int)\n",
    "    return x,y\n",
    "\n",
    "def measure_next(a,new) -> np.ndarray:\n",
    "    \"\"\" get the value of the next point\"\"\"\n",
    "    return a[:,new[0],new[1]]\n",
    "    # newval = [new[0]*map_shape[1]+new[1]]\n",
    "    # assert np.allclose(new,newval[:2]),\"coordinates don't match\"\n",
    "    # points.append(new)\n",
    "    # values.append(a[new[0]*250+new[1]])\n",
    "    # return newval[2:]\n",
    "    # new,a[new[0]*250+new[1]][2]\n",
    "\n",
    "def run_gp(init_points,init_values,device,method,dask_client=None):\n",
    "    \"\"\"Run the GP for one iteration.\n",
    "    \n",
    "    Args:\n",
    "        points (np.ndarray): The points that have been measured so far.\n",
    "        values (np.ndarray): The values that have been measured so far.\n",
    "        gp (GaussianProcess): The GP that is used to model the data.\n",
    "        device (str): The device that is used for the GP.\n",
    "        method (str): The method that is used for the GP.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[np.ndarray,np.ndarray]\n",
    "    \"\"\"\n",
    "    gp = init_gp(\n",
    "        init_points,init_values,index_set_bounds,hyperparameter_bounds,hps_guess,vp,\n",
    "        device=device,\n",
    "    )\n",
    "    # info = []\n",
    "    times = []\n",
    "    points = points.copy()\n",
    "    values = values.copy()\n",
    "    for i in tqdm(range(n_iterations)):\n",
    "        try:\n",
    "            t0 = time.time()\n",
    "            newpoint = find_next(\n",
    "                points,\n",
    "                values,\n",
    "                gp,\n",
    "                method=method,\n",
    "                hyperparameter_bounds=hyperparameter_bounds,\n",
    "                dask_client=dask_client,\n",
    "            )\n",
    "            newval = measure_next(reduced_data,newpoint)\n",
    "            values = np.append(values,np.array(newval)[None,:],axis=0)\n",
    "            points = np.append(points,np.array(newpoint)[None,:],axis=0)\n",
    "            times.append(time.time()-t0)\n",
    "            # ig = gp.shannon_information_gain(np.append(points[-1],np.array([0])))['prior entropy']\n",
    "            # ig += gp.shannon_information_gain(np.append(points[-1],np.array([1])))['prior entropy']\n",
    "            # info.append(ig)\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "    return points, values, times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load SGM4 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file =  Path(r\"D:\\data\\SGM4\\SmartScan\\Z006_35_0.h5\")\n",
    "ldr = dl.load(source_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata = ldr.to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_variance(data):\n",
    "    # [hist_variance(xdata.values[i,j,...]) for i,j in product(range(xdata.shape[0]),range(xdata.shape[1]))] \n",
    "    hist = np.histogram(data,bins=100)\n",
    "    return np.sqrt(np.var(hist[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.ndimage import gaussian_filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_map(xdata,func,**kwargs) -> np.ndarray:\n",
    "    out = np.zeros(xdata.shape[:2])\n",
    "    pts = np.array(list(product(range(xdata.shape[0]),range(xdata.shape[1]))))\n",
    "    for i,j in tqdm(pts,leave=False):\n",
    "        out[i,j,...] = func(xdata.values[i,j,...],**kwargs)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_entropy(data):\n",
    "    hist = np.histogram(data/data.sum(),bins=100)\n",
    "    return entropy(hist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_gradient_entropy(data):\n",
    "    vals = data.ravel()\n",
    "    tol = 0\n",
    "    grad = np.gradient(vals[vals>tol])\n",
    "    smooth_grad = gaussian_filter(grad,sigma=3)\n",
    "    hist = np.histogram(smooth_grad,bins=100)\n",
    "    return entropy(hist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(10,5),layout='constrained')\n",
    "# def \n",
    "vals = xdata.values[20,20,...]\n",
    "tol = 0\n",
    "# squared sum of gradient\n",
    "grad = np.sqrt(np.power(np.gradient(vals)[0],2.)+np.power(np.gradient(vals)[1],2))\n",
    "# grad = np.gradient(vals)[0]\n",
    "# grad[vals<tol] = 0\n",
    "smooth_grad = gaussian_filter(grad,sigma=5)\n",
    "smooth_vals = gaussian_filter(vals,sigma=5)\n",
    "ax[0].imshow(smooth_vals)\n",
    "ax[1].imshow(smooth_grad)#/smooth_grad.mean() - smooth_vals/smooth_vals.mean())#.reshape(xdata.shape[2:]))\n",
    "# hist = np.histogram(smooth_grad,bins=100)\n",
    "ax[2].hist(smooth_grad.ravel(),bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata.values[20,20,...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals[vals>tol].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = np.array([\n",
    "    xdata.mean(['Kinetic Energy','OrdinateRange']),\n",
    "    # eval_map(xdata,hist_variance),\n",
    "    eval_map(xdata,hist_gradient_entropy),\n",
    "    # xdata.std(['Kinetic Energy','OrdinateRange']),\n",
    "    \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(10,5))\n",
    "ax[0].imshow(reduced_data[0])\n",
    "ax[1].imshow(reduced_data[1])\n",
    "# add colorbars\n",
    "fig.colorbar(ax[0].imshow(reduced_data[0]),ax=ax[0])\n",
    "fig.colorbar(ax[1].imshow(reduced_data[1]),ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test variance evaluation of focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_1 = np.random.randint(0,xdata.FSamX.size), np.random.randint(0,xdata.FSamY.size)\n",
    "idx_2 = np.random.randint(0,xdata.FSamX.size), np.random.randint(0,xdata.FSamY.size)\n",
    "print(idx_1,idx_2)\n",
    "img1 = xdata.isel(FSamX=idx_1[0],FSamY=idx_1[1]).values\n",
    "img2 = xdata.isel(FSamX=idx_2[0],FSamY=idx_2[1]).values\n",
    "fig,ax = plt.subplots(1,2)\n",
    "ax[0].imshow(img1)\n",
    "ax[1].imshow(img2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.hist((img1/img1.sum()).ravel(),bins=100,color='r',alpha=.5,label='1')\n",
    "ax.hist((img2/img2.sum()).ravel(),bins=100,color='b',alpha=.5,label='2')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.set_ylim(1e2,1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram variance\n",
    "img_1_hist = np.histogram(img1,bins=100)\n",
    "img_1_var = np.sqrt(np.var(img1))\n",
    "img_2_var = np.sqrt(np.var(img2))\n",
    "print(img_1_var,img_2_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_values = reduced_data.reshape(2,-1).T\n",
    "all_points = np.array(tuple(product(range(xdata.shape[0]),range(xdata.shape[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_points) == len(all_values) == np.prod(xdata.shape[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(10,5))\n",
    "ax[0].imshow(reduced_data[0])\n",
    "ax[1].imshow(reduced_data[1])\n",
    "# add colorbars\n",
    "fig.colorbar(ax[0].imshow(reduced_data[0]),ax=ax[0])\n",
    "fig.colorbar(ax[1].imshow(reduced_data[1]),ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(15,5))\n",
    "ax[0].imshow(xdata.values[20,20,...])\n",
    "ax[1].imshow(xdata.values[30,20,...])\n",
    "ax[2].imshow(xdata.values[50,15,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_shape = reduced_data.shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rng = default_rng()\n",
    "ind = rng.choice(len(all_points)-1, size=5, replace=False)\n",
    "init_points = all_points[ind]\n",
    "init_values = all_values[ind]\n",
    "print(f\"init_points: {len(init_points)} {init_points[0]} -> {init_points[-1]}\")\n",
    "print(f\"init_values: {len(init_values)} {init_values[0]} -> {init_values[-1]}\")\n",
    "print(\"x_min \", np.min(init_points[:,0]),\" x_max \",np.max(init_points[:,0]))\n",
    "print(\"y_min \", np.min(init_points[:,1]),\" y_max \",np.max(init_points[:,1]))\n",
    "print(\"val_min \", np.min(init_values[:,1]),\" val_max \",np.max(init_values[:,1]))\n",
    "print(\"length of data set: \", len(init_values))\n",
    "\n",
    "index_set_bounds = np.array([[0,map_shape[0]-1],[0,map_shape[1]-1]])\n",
    "hyperparameter_bounds = np.array([[0.001,1e9],[1,1000],[1,1000],[1,1000],[1,1000]])\n",
    "hps_guess = np.array([4.71907062e+06, 4.07439017e+02, 3.59068120e+02,4e2,4e2])\n",
    "\n",
    "z=[[[0],[1]]]\n",
    "vp = np.array(z*len(ind))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(10,5))\n",
    "ax[0].imshow(reduced_data[0])\n",
    "ax[1].imshow(reduced_data[1])\n",
    "ax[0].scatter(init_points[:,1],init_points[:,0],c='r')\n",
    "ax[1].scatter(init_points[:,1],init_points[:,0],c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_cpu_g, times_cpu_g, points_cpu_g, values_cpu_g = run_gp(init_points,init_values,'cpu','global')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(8,4),sharex=True,sharey=True,layout='constrained')\n",
    "\n",
    "ax.imshow(reduced_data[0,...],alpha=0.5,cmap='Reds')\n",
    "ax.imshow(reduced_data[1,...],alpha=0.5,cmap='Blues')\n",
    "ax.scatter(init_points[:,1],init_points[:,0],c='k',marker='o',label='initial points')\n",
    "ax.scatter(points_cpu_g[:,1],points_cpu_g[:,0],c='r',marker='x',label='cpu global')\n",
    "ax.set_title('cpu global')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_cpu_g, times_cpu_g, points_cpu_g, values_cpu_g = run_gp(init_points,init_values,'cpu','global')\n",
    "info_cpu_l, times_cpu_l, points_cpu_l, values_cpu_l = run_gp(init_points,init_values,'cpu','local')\n",
    "info_gpu_g, times_gpu_g, points_gpu_g, values_gpu_g = run_gp(init_points,init_values,'gpu','global')\n",
    "info_gpu_l, times_gpu_l, points_gpu_l, values_gpu_l = run_gp(init_points,init_values,'gpu','local')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(8,4))\n",
    "ax[0].plot(times_cpu_g,label='cpu global')\n",
    "ax[0].plot(times_cpu_l,label='cpu local')\n",
    "ax[0].plot(times_gpu_g,label='gpu global')\n",
    "ax[0].plot(times_gpu_l,label='gpu local')\n",
    "ax[0].legend()\n",
    "ax[0].set_title('time per iteration')\n",
    "ax[0].set_xlabel('iteration')\n",
    "ax[0].set_ylabel('time [s]')\n",
    "# reset colors\n",
    "ax[1].set_prop_cycle(None)\n",
    "ax[1].plot(np.cumsum(times_cpu_g),info_cpu_g,label='cpu global')\n",
    "ax[1].plot(np.cumsum(times_cpu_l),info_cpu_l,label='cpu local')\n",
    "ax[1].plot(np.cumsum(times_gpu_g),info_gpu_g,label='gpu global')\n",
    "ax[1].plot(np.cumsum(times_gpu_l),info_gpu_l,label='gpu local')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel('time [s]')\n",
    "ax[1].set_ylabel('information gain')\n",
    "ax[1].set_title('cumulative time vs. information gain')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(2,2,figsize=(8,4),sharex=True,sharey=True,layout='constrained')\n",
    "for ax in axes.flatten():\n",
    "    ax.imshow(a[:,2].reshape(map_shape),alpha=0.5,cmap='Reds')\n",
    "    ax.imshow(a[:,3].reshape(map_shape),alpha=0.5,cmap='Blues')\n",
    "axes[0,0].scatter(init_points[:,1],init_points[:,0],c='k',marker='x',label='initial points')\n",
    "axes[0,0].scatter(points_cpu_g[:,1],points_cpu_g[:,0],c='r',marker='x',label='cpu global')\n",
    "axes[0,0].set_title('cpu global')\n",
    "\n",
    "axes[0,1].scatter(points_cpu_l[:,1],points_cpu_l[:,0],c='b',marker='x',label='cpu local')\n",
    "axes[0,1].set_title('cpu local')\n",
    "\n",
    "axes[1,0].scatter(points_gpu_g[:,1],points_gpu_g[:,0],c='r',marker='x',label='gpu global')\n",
    "axes[1,0].set_title('gpu global')\n",
    "\n",
    "axes[1,1].scatter(points_gpu_l[:,1],points_gpu_l[:,0],c='b',marker='x',label='gpu local')\n",
    "axes[1,1].set_title('gpu local')\n",
    "\n",
    "    \n",
    "# ax[0,0].imshow(values_cpu_g[:,0].reshape(100*3,250*3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement dask client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask\n",
    "# import dask.array as da\n",
    "# import dask.dataframe as dd\n",
    "# dask client/server\n",
    "from dask.distributed import Client, LocalCluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_server = LocalCluster(n_workers=4,threads_per_worker=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_cpu_g_dask, times_cpu_g_dask, points_cpu_g_dask, values_cpu_g_dask = run_gp(init_points,init_values,'cpu','global',dask_client=dask_server)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_cpu_g_dask_8, times_cpu_g_dask_8, points_cpu_g_dask_8, values_cpu_g_dask_8 = run_gp(init_points,init_values,'cpu','global',dask_client=dask_server)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_cpu_g_dask_16, times_cpu_g_dask_16, points_cpu_g_dask_16, values_cpu_g_dask_16 = run_gp(init_points,init_values,'cpu','global',dask_client=dask_server)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_cpu_g_dask_1, times_cpu_g_dask_1, points_cpu_g_dask_1, values_cpu_g_dask_1 = run_gp(init_points,init_values,'cpu','global',dask_client=dask_server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_cpu_g_dask_1, times_cpu_g_dask_1, points_cpu_g_dask_1, values_cpu_g_dask_1 = run_gp(init_points,init_values,'gpu','global',dask_client=dask_server)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [\n",
    "    times_cpu_g,\n",
    "    times_cpu_g_dask,\n",
    "    times_cpu_g_dask_8,\n",
    "    times_cpu_g_dask_16,\n",
    "    times_cpu_g_dask_1,\n",
    "    times_cpu_l,\n",
    "    times_gpu_g,\n",
    "    times_gpu_l,\n",
    "]\n",
    "infos = [\n",
    "    info_cpu_g,\n",
    "    info_cpu_g_dask,\n",
    "    info_cpu_g_dask_8,\n",
    "    info_cpu_g_dask_16,\n",
    "    info_cpu_g_dask_1,\n",
    "    info_cpu_l,\n",
    "    info_gpu_g,\n",
    "    info_gpu_l,\n",
    "]\n",
    "labels = [\n",
    "    'cpu global',\n",
    "    'cpu global dask',\n",
    "    'cpu global dask 8',\n",
    "    'cpu global dask 16',\n",
    "    'cpu global dask 1',\n",
    "    'cpu local',\n",
    "    'gpu global',\n",
    "    'gpu local',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter, butter, filtfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(8,4))\n",
    "for i,time_,info,label in zip(range(len(times)),times,infos,labels):\n",
    "    if i == 5:\n",
    "        break\n",
    "    l = ax[0].plot(time_,'.',alpha=0.5)\n",
    "    # plot filtered with same color\n",
    "    time_filt = savgol_filter(time_,51,3)\n",
    "    time_filt = filtfilt(*butter(3,.3),time_)\n",
    "    ax[0].plot(time_filt,label=label,c=l[0].get_color(),alpha=1)\n",
    "    ax[1].plot(np.cumsum(time_),info,'.',alpha=.5)\n",
    "    ax[1].plot(np.cumsum(time_filt),info,label=label,c=l[0].get_color(),alpha=1)\n",
    "\n",
    "ax[0].legend(fontsize='x-small')\n",
    "ax[0].set_title('time per iteration')\n",
    "ax[0].set_xlabel('iteration')\n",
    "ax[0].set_ylabel('time [s]')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel('time [s]')\n",
    "ax[1].set_ylabel('information gain')\n",
    "ax[1].set_title('cumulative time vs. information gain')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
